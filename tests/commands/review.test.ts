import { describe, it, beforeEach, afterEach, expect, vi } from 'vitest';

// Mock ESM modules
vi.mock('@riotprompt/riotprompt', () => {
    const localBuilder: any = {
        addPersonaPath: vi.fn(async () => localBuilder),
        addInstructionPath: vi.fn(async () => localBuilder),
        addContent: vi.fn(async () => localBuilder),
        loadContext: vi.fn(async () => localBuilder),
        addContext: vi.fn(async () => localBuilder),
        build: vi.fn().mockResolvedValue('mock prompt')
    };

    return {
        createSection: vi.fn().mockReturnValue({
            add: vi.fn()
        }),
        Model: {
            GPT_4: 'gpt-4'
        },
        Formatter: {
            create: vi.fn().mockReturnValue({
                formatPrompt: vi.fn().mockReturnValue({ messages: [] })
            })
        },
        Builder: {
            create: vi.fn(() => localBuilder)
        },
        recipe: vi.fn().mockImplementation(() => ({
            persona: vi.fn().mockImplementation(() => ({
                instructions: vi.fn().mockImplementation(() => ({
                    overridePaths: vi.fn().mockImplementation(() => ({
                        overrides: vi.fn().mockImplementation(() => ({
                            content: vi.fn().mockImplementation(() => ({
                                context: vi.fn().mockImplementation(() => ({
                                    cook: vi.fn().mockResolvedValue('mock prompt')
                                }))
                            }))
                        }))
                    }))
                }))
            }))
        }))
    };
});

const mockCreatePrompt = vi.fn().mockResolvedValue('mock prompt');
vi.mock('../../src/prompt/review', () => ({
    createPrompt: mockCreatePrompt
}));

const mockLogGet = vi.fn().mockResolvedValue('mock log content');
const mockLogCreate = vi.fn().mockReturnValue({
    get: mockLogGet
});
vi.mock('../../src/content/log', () => ({
    create: mockLogCreate
}));

const mockDiffGetRecentDiffsForReview = vi.fn().mockResolvedValue('mock diff content');
const mockDiffGetReviewExcludedPatterns = vi.fn().mockReturnValue(['*.test.ts', '*.spec.ts']);
vi.mock('../../src/content/diff', () => ({
    getRecentDiffsForReview: mockDiffGetRecentDiffsForReview,
    getReviewExcludedPatterns: mockDiffGetReviewExcludedPatterns
}));

const mockReleaseNotesGet = vi.fn().mockResolvedValue('mock release notes');
vi.mock('../../src/content/releaseNotes', () => ({
    get: mockReleaseNotesGet
}));

const mockIssuesGet = vi.fn().mockResolvedValue('mock issues content');
const mockIssuesHandleIssueCreation = vi.fn().mockResolvedValue('Issues created successfully');
vi.mock('../../src/content/issues', () => ({
    get: mockIssuesGet,
    handleIssueCreation: mockIssuesHandleIssueCreation
}));

const mockCreateCompletion = vi.fn().mockResolvedValue({
    summary: 'Review analysis summary',
    totalIssues: 3,
    issues: [
        { title: 'Issue 1', priority: 'high' },
        { title: 'Issue 2', priority: 'medium' },
        { title: 'Issue 3', priority: 'low' }
    ]
});
vi.mock('../../src/util/openai', () => ({
    createCompletion: mockCreateCompletion,
    getModelForCommand: vi.fn().mockImplementation((config, commandName) => {
        if (commandName === 'review' && config.review?.model) {
            return config.review.model;
        }
        return config.model || 'gpt-4o-mini';
    }),
    getOpenAIReasoningForCommand: vi.fn().mockReturnValue('low'),
    getOpenAIMaxOutputTokensForCommand: vi.fn().mockReturnValue(10000)
}));

const mockLogger = {
    info: vi.fn(),
    debug: vi.fn(),
    warn: vi.fn(),
    error: vi.fn()
};
vi.mock('../../src/logging', () => ({
    getLogger: vi.fn().mockReturnValue(mockLogger)
}));

vi.mock('../../src/util/general', () => ({
    getOutputPath: vi.fn().mockImplementation((dir, file) => `${dir}/${file}`),
    getTimestampedRequestFilename: vi.fn().mockReturnValue('request-123456.json'),
    getTimestampedResponseFilename: vi.fn().mockReturnValue('response-123456.json'),
    getTimestampedReviewFilename: vi.fn().mockReturnValue('review-123456.md'),
    getTimestampedReviewNotesFilename: vi.fn().mockReturnValue('review-notes-123456.md')
}));

const mockStorage = {
    ensureDirectory: vi.fn().mockResolvedValue(undefined),
    writeFile: vi.fn().mockResolvedValue(undefined)
};
const mockCreateStorage = vi.fn().mockReturnValue(mockStorage);
vi.mock('../../src/util/storage', () => ({
    create: mockCreateStorage
}));

// Mock interactive utilities
const mockGetUserChoice = vi.fn().mockResolvedValue('c');
vi.mock('../../src/util/interactive', () => ({
    getUserChoice: mockGetUserChoice
}));

vi.mock('../../src/constants', () => ({
    DEFAULT_EXCLUDED_PATTERNS: ['node_modules', '*.test.ts'],
    DEFAULT_OUTPUT_DIRECTORY: 'output'
}));

// Mock Node.js built-in modules
vi.mock('path', () => {
    const pathMock = {
        join: vi.fn().mockImplementation((...args) => args.join('/')),
        dirname: vi.fn().mockImplementation((p) => p.split('/').slice(0, -1).join('/') || '/')
    };
    return {
        default: pathMock,
        ...pathMock
    };
});

vi.mock('os', () => ({
    default: {
        tmpdir: vi.fn().mockReturnValue('/tmp')
    }
}));

vi.mock('child_process', () => ({
    spawnSync: vi.fn(),
    spawn: vi.fn()
}));

vi.mock('fs/promises', () => ({
    default: {
        writeFile: vi.fn().mockResolvedValue(undefined),
        readFile: vi.fn().mockResolvedValue('Test review note content'),
        readdir: vi.fn().mockResolvedValue([]),
        unlink: vi.fn().mockResolvedValue(undefined),
        access: vi.fn().mockResolvedValue(undefined),
        open: vi.fn().mockResolvedValue({
            close: vi.fn().mockResolvedValue(undefined)
        })
    }
}));

vi.mock('fs', () => {
    const fsMock = {
        constants: {
            W_OK: 2
        }
    };
    return {
        default: fsMock,
        ...fsMock
    };
});

// Mock process.exit to prevent actual exit during tests
const mockExit = vi.spyOn(process, 'exit').mockImplementation(() => {
    throw new Error('process.exit called');
});

describe('review command', () => {
    let Review: any;
    let mockProcess: any;
    let mockSpawnSync: any;
    let mockFs: any;

    beforeEach(async () => {
        // Reset all mocks before each test
        vi.clearAllMocks();
        mockExit.mockClear();

        // Reset mock implementations
        mockCreatePrompt.mockResolvedValue('mock prompt');
        mockLogGet.mockResolvedValue('mock log content');
        mockDiffGetRecentDiffsForReview.mockResolvedValue('mock diff content');
        mockReleaseNotesGet.mockResolvedValue('mock release notes');
        mockIssuesGet.mockResolvedValue('mock issues content');
        mockCreateCompletion.mockResolvedValue({
            summary: 'Review analysis summary',
            totalIssues: 3,
            issues: [
                { title: 'Issue 1', priority: 'high' },
                { title: 'Issue 2', priority: 'medium' },
                { title: 'Issue 3', priority: 'low' }
            ]
        });
        mockIssuesHandleIssueCreation.mockResolvedValue('Issues created successfully');
        mockStorage.ensureDirectory.mockResolvedValue(undefined);
        mockStorage.writeFile.mockResolvedValue(undefined);
        mockDiffGetReviewExcludedPatterns.mockReturnValue(['*.test.ts', '*.spec.ts']);
        mockGetUserChoice.mockResolvedValue('c');

        // Mock process stdin
        mockProcess = {
            stdin: { isTTY: true },
            env: { EDITOR: 'vi' },
            exit: mockExit
        };
        globalThis.process = mockProcess as any;

        // Import modules after mocking
        const childProcess = await import('child_process');
        mockSpawnSync = childProcess.spawnSync;
        mockSpawnSync.mockReturnValue({ error: null });

        // Mock spawn for editor functionality
        const mockSpawn = childProcess.spawn as any;
        const mockChild = {
            on: vi.fn().mockImplementation((event, callback) => {
                if (event === 'exit') {
                    setTimeout(() => callback(0), 0);
                }
            }),
            kill: vi.fn()
        };
        mockSpawn.mockReturnValue(mockChild);

        // Initialize fs mock
        const fs = await import('fs/promises');
        mockFs = fs.default;

        // Re-establish fs mocks after clearing
        mockFs.writeFile.mockResolvedValue(undefined);
        mockFs.readFile.mockResolvedValue('Test review note content');
        mockFs.readdir.mockResolvedValue([
            { name: 'file1.md', isFile: () => true },
            { name: 'file2.md', isFile: () => true }
        ]);
        mockFs.unlink.mockResolvedValue(undefined);
        mockFs.access.mockResolvedValue(undefined);
        mockFs.open.mockResolvedValue({
            close: vi.fn().mockResolvedValue(undefined)
        });

        Review = await import('../../src/commands/review');
    });

    afterEach(() => {
        vi.clearAllMocks();
    });

    describe('dry run mode', () => {
        it('should log configuration and return without executing when in dry run mode', async () => {
            const runConfig = {
                model: 'gpt-4',
                dryRun: true,
                review: {
                    note: 'Test review note',
                    includeCommitHistory: true,
                    includeRecentDiffs: true,
                    includeReleaseNotes: true,
                    includeGithubIssues: true,
                    commitHistoryLimit: 10,
                    diffHistoryLimit: 5,
                    releaseNotesLimit: 3,
                    githubIssuesLimit: 20,
                    sendit: true
                }
            };

            const result = await Review.execute(runConfig);

            expect(mockLogger.info).toHaveBeenCalledWith('DRY RUN: Would analyze provided note for review');
            expect(mockLogger.info).toHaveBeenCalledWith('DRY RUN: Would gather additional context based on configuration above');
            expect(mockLogger.info).toHaveBeenCalledWith('DRY RUN: Would analyze note and identify issues');
            expect(mockLogger.info).toHaveBeenCalledWith('DRY RUN: Would automatically create GitHub issues (sendit mode enabled)');
            expect(mockLogger.debug).toHaveBeenCalledWith('Review context configuration:');
            expect(result).toBe('DRY RUN: Review command would analyze note, gather context, and create GitHub issues');
        });

        it('should show exclusion patterns in dry run mode when including recent diffs', async () => {
            const runConfig = {
                model: 'gpt-4',
                dryRun: true,
                review: {
                    includeRecentDiffs: true
                }
            };

            await Review.execute(runConfig);

            expect(mockDiffGetReviewExcludedPatterns).toHaveBeenCalledWith(['node_modules', '*.test.ts']);
            expect(mockLogger.info).toHaveBeenCalledWith('DRY RUN: Would use %d exclusion patterns for diff context', 2);
        });

        it('should show prompt mode instead of sendit mode in dry run', async () => {
            const runConfig = {
                model: 'gpt-4',
                dryRun: true,
                review: {
                    sendit: false
                }
            };

            await Review.execute(runConfig);

            expect(mockLogger.info).toHaveBeenCalledWith('DRY RUN: Would prompt for confirmation before creating GitHub issues');
        });
    });

    describe('STDIN validation', () => {
        it('should throw error when STDIN is piped but sendit is not enabled', async () => {
            mockProcess.stdin.isTTY = false;
            const runConfig = {
                model: 'gpt-4',
                review: {
                    sendit: false
                }
            };

            await expect(Review.execute(runConfig)).rejects.toThrow('Piped input requires --sendit flag for non-interactive operation');
            expect(mockLogger.error).toHaveBeenCalledWith('❌ STDIN is piped but --sendit flag is not enabled');
        });

        it('should not throw error when STDIN is piped and sendit is enabled', async () => {
            mockProcess.stdin.isTTY = false;
            const runConfig = {
                model: 'gpt-4',
                review: {
                    note: 'Test review note',
                    sendit: true
                }
            };

            const result = await Review.execute(runConfig);

            expect(result).toBe('Issues created successfully');
            expect(mockLogger.error).not.toHaveBeenCalled();
        });
    });

    describe('review note gathering', () => {
        it('should use review note from configuration when provided', async () => {
            const runConfig = {
                model: 'gpt-4',
                review: {
                    note: 'Test review note from config',
                    sendit: true
                }
            };

            await Review.execute(runConfig);

            expect(mockLogger.debug).toHaveBeenCalledWith('Review note: %s', 'Test review note from config');
            expect(mockSpawnSync).not.toHaveBeenCalled();
        });

        it('should open editor when no review note is provided', async () => {
            const runConfig = {
                model: 'gpt-4',
                review: {
                    sendit: true
                }
            };

            await Review.execute(runConfig);

            // Check that temp file was created (look for the actual temp file, not .test files)
            const tempFileCall = mockFs.writeFile.mock.calls.find(
                (call: any) => call[0].includes('kodrdriv_review_') && !call[0].includes('.test')
            );
            expect(tempFileCall).toBeDefined();
            expect(tempFileCall![1]).toContain('# Kodrdriv Review Note');
            expect(tempFileCall![2]).toBe('utf-8');

            // Check that spawn was called (not spawnSync)
            const childProcess = await import('child_process');
            const mockSpawn = childProcess.spawn;
            expect(mockSpawn).toHaveBeenCalledWith('vi', [expect.stringMatching(/^\/tmp\/kodrdriv_review_\d+_[\w]+\.md$/)], {
                stdio: 'inherit',
                shell: false
            });

            expect(mockFs.readFile).toHaveBeenCalledWith(expect.stringMatching(/^\/tmp\/kodrdriv_review_\d+_[\w]+\.md$/), 'utf8');
            expect(mockFs.unlink).toHaveBeenCalledWith(expect.stringMatching(/^\/tmp\/kodrdriv_review_\d+_[\w]+\.md$/));
        });

        it('should use custom editor from environment variables', async () => {
            mockProcess.env.EDITOR = 'nano';
            const runConfig = {
                model: 'gpt-4',
                review: {
                    sendit: true
                }
            };

            await Review.execute(runConfig);

            // Now uses spawn instead of spawnSync - get the spawn mock
            const childProcess = await import('child_process');
            const mockSpawn = childProcess.spawn;
                        expect(mockSpawn).toHaveBeenCalledWith('nano', [expect.stringMatching(/^\/tmp\/kodrdriv_review_\d+_[\w]+\.md$/)], {
                stdio: 'inherit',
                shell: false
            });
        });

        it('should throw error when editor fails to launch', async () => {
            // Mock spawn to emit an error event
            const childProcess = await import('child_process');
            const mockSpawn = childProcess.spawn as any;
            const mockChild = {
                on: vi.fn().mockImplementation((event, callback) => {
                    if (event === 'error') {
                        setTimeout(() => callback(new Error('Editor not found')), 0);
                    }
                }),
                kill: vi.fn()
            };
            mockSpawn.mockReturnValue(mockChild);

            const runConfig = {
                model: 'gpt-4',
                review: {
                    sendit: true
                }
            };

            await expect(Review.execute(runConfig)).rejects.toThrow('Failed to launch editor \'vi\': Editor not found');
        });

        it('should throw error when editor returns empty content', async () => {
            mockFs.readFile.mockResolvedValue('# Only comments\n# No actual content');
            const runConfig = {
                model: 'gpt-4',
                review: {
                    sendit: true
                }
            };

            await expect(Review.execute(runConfig)).rejects.toThrow('Review note is empty – aborting. Provide a note as an argument, via STDIN, or through the editor.');
        });

        it('should filter out comment lines from editor content', async () => {
            mockFs.readFile.mockResolvedValue('# This is a comment\nActual review note\n# Another comment\nMore content');
            const runConfig = {
                model: 'gpt-4',
                review: {
                    sendit: true
                }
            };

            await Review.execute(runConfig);

            expect(mockLogger.debug).toHaveBeenCalledWith('Review note: %s', 'Actual review note\nMore content');
        });
    });

    describe('context gathering', () => {
        it('should gather commit history when enabled', async () => {
            const runConfig = {
                model: 'gpt-4',
                review: {
                    note: 'Test note',
                    includeCommitHistory: true,
                    commitHistoryLimit: 10,
                    sendit: true
                }
            };

            await Review.execute(runConfig);

            expect(mockLogCreate).toHaveBeenCalledWith({ limit: 10 });
            expect(mockLogGet).toHaveBeenCalled();
            expect(mockLogger.debug).toHaveBeenCalledWith('Fetching recent commit history...');
        });

        it('should gather recent diffs when enabled', async () => {
            const runConfig = {
                model: 'gpt-4',
                review: {
                    note: 'Test note',
                    includeRecentDiffs: true,
                    diffHistoryLimit: 5,
                    sendit: true
                }
            };

            await Review.execute(runConfig);

            expect(mockDiffGetRecentDiffsForReview).toHaveBeenCalledWith({
                limit: 5,
                baseExcludedPatterns: ['node_modules', '*.test.ts']
            });
            expect(mockLogger.debug).toHaveBeenCalledWith('Fetching recent commit diffs...');
        });

        it('should gather release notes when enabled', async () => {
            const runConfig = {
                model: 'gpt-4',
                review: {
                    note: 'Test note',
                    includeReleaseNotes: true,
                    releaseNotesLimit: 3,
                    sendit: true
                }
            };

            await Review.execute(runConfig);

            expect(mockReleaseNotesGet).toHaveBeenCalledWith({ limit: 3 });
            expect(mockLogger.debug).toHaveBeenCalledWith('Fetching recent release notes from GitHub...');
        });

        it('should gather GitHub issues when enabled', async () => {
            const runConfig = {
                model: 'gpt-4',
                review: {
                    note: 'Test note',
                    includeGithubIssues: true,
                    githubIssuesLimit: 20,
                    sendit: true
                }
            };

            await Review.execute(runConfig);

            expect(mockIssuesGet).toHaveBeenCalledWith({ limit: 20 });
            expect(mockLogger.debug).toHaveBeenCalledWith('Fetching open GitHub issues...');
        });

        it('should handle errors in context gathering gracefully', async () => {
            mockLogGet.mockRejectedValue(new Error('Git log failed'));
            mockDiffGetRecentDiffsForReview.mockRejectedValue(new Error('Diff failed'));
            mockReleaseNotesGet.mockRejectedValue(new Error('Release notes failed'));
            mockIssuesGet.mockRejectedValue(new Error('Issues failed'));

            const runConfig = {
                model: 'gpt-4',
                review: {
                    note: 'Test note',
                    includeCommitHistory: true,
                    includeRecentDiffs: true,
                    includeReleaseNotes: true,
                    includeGithubIssues: true,
                    sendit: true
                }
            };

            // Should not throw, should handle errors gracefully
            await Review.execute(runConfig);

            expect(mockLogger.warn).toHaveBeenCalledWith('Failed to fetch commit history: Git log failed');
            expect(mockLogger.warn).toHaveBeenCalledWith('Failed to fetch recent diffs: Diff failed');
            expect(mockLogger.warn).toHaveBeenCalledWith('Failed to fetch release notes: Release notes failed');
            expect(mockLogger.warn).toHaveBeenCalledWith('Failed to fetch GitHub issues: Issues failed');
            expect(mockLogger.warn).toHaveBeenCalledWith('Context gathering completed with 4 error(s):');
            expect(mockLogger.warn).toHaveBeenCalledWith('  - Failed to fetch commit history: Git log failed');
            expect(mockLogger.warn).toHaveBeenCalledWith('  - Failed to fetch recent diffs: Diff failed');
            expect(mockLogger.warn).toHaveBeenCalledWith('  - Failed to fetch release notes: Release notes failed');
            expect(mockLogger.warn).toHaveBeenCalledWith('  - Failed to fetch GitHub issues: Issues failed');
        });

        it('should use custom excluded patterns for diffs', async () => {
            const runConfig = {
                model: 'gpt-4',
                excludedPatterns: ['*.custom.ts', '*.spec.ts'],
                review: {
                    note: 'Test note',
                    includeRecentDiffs: true,
                    sendit: true
                }
            };

            await Review.execute(runConfig);

            expect(mockDiffGetRecentDiffsForReview).toHaveBeenCalledWith({
                limit: undefined,
                baseExcludedPatterns: ['*.custom.ts', '*.spec.ts']
            });
        });
    });

    describe('OpenAI analysis', () => {
        it('should analyze review note and create completion', async () => {
            const runConfig = {
                model: 'gpt-4',
                review: {
                    note: 'Test review note',
                    context: 'Additional context',
                    sendit: true
                }
            };

            const result = await Review.execute(runConfig);

            expect(mockCreatePrompt).toHaveBeenCalledWith(
                expect.objectContaining({
                    overridePaths: [],
                    overrides: false
                }),
                expect.objectContaining({
                    notes: 'Test review note'
                }),
                expect.objectContaining({
                    context: 'Additional context'
                })
            );

            expect(mockCreateCompletion).toHaveBeenCalledWith(
                [],
                expect.objectContaining({
                    model: 'gpt-4',
                    responseFormat: { type: 'json_object' }
                })
            );

            expect(result).toBe('Issues created successfully');
        });

        it('should handle debug mode with debug files', async () => {
            const runConfig = {
                model: 'gpt-4',
                debug: true,
                outputDirectory: 'debug-output',
                review: {
                    note: 'Test note',
                    sendit: true
                }
            };

            await Review.execute(runConfig);

            expect(mockCreateCompletion).toHaveBeenCalledWith(
                [],
                expect.objectContaining({
                    debug: true,
                    debugRequestFile: 'debug-output/request-123456.json',
                    debugResponseFile: 'debug-output/response-123456.json'
                })
            );
        });

        it('should pass discovered config directories and overrides', async () => {
            const runConfig = {
                model: 'gpt-4',
                discoveredConfigDirs: ['config1', 'config2'],
                overrides: true,
                review: {
                    note: 'Test note',
                    sendit: true
                }
            };

            await Review.execute(runConfig);

            expect(mockCreatePrompt).toHaveBeenCalledWith(
                expect.objectContaining({
                    overridePaths: ['config1', 'config2'],
                    overrides: true
                }),
                expect.any(Object),
                expect.any(Object)
            );
        });

        it('should log analysis result details', async () => {
            const runConfig = {
                model: 'gpt-4',
                review: {
                    note: 'Test note',
                    sendit: true
                }
            };

            await Review.execute(runConfig);

            expect(mockLogger.debug).toHaveBeenCalledWith('Analysis result summary: %s', 'Review analysis summary');
            expect(mockLogger.debug).toHaveBeenCalledWith('Total issues found: %d', 3);
            expect(mockLogger.debug).toHaveBeenCalledWith('Issues array length: %d', 3);
        });
    });

    describe('file saving', () => {
        it('should save timestamped review notes and analysis results', async () => {
            const runConfig = {
                model: 'gpt-4',
                review: {
                    note: 'Test review note',
                    context: 'User context',
                    sendit: true
                }
            };

            await Review.execute(runConfig);

            // Check review notes file - now uses fs.writeFile directly via safeWriteFile
            expect(mockFs.writeFile).toHaveBeenCalledWith(
                'output/review-notes-123456.md',
                expect.stringContaining('# Review Notes\n\nTest review note'),
                'utf-8'
            );

            // Check analysis result file - now uses fs.writeFile directly via safeWriteFile
            expect(mockFs.writeFile).toHaveBeenCalledWith(
                'output/review-123456.md',
                expect.stringContaining('# Review Analysis Result'),
                'utf-8'
            );
        });

        it('should include all context sections in review notes file', async () => {
            mockLogGet.mockResolvedValue('commit history content');
            mockDiffGetRecentDiffsForReview.mockResolvedValue('diff content');
            mockReleaseNotesGet.mockResolvedValue('release notes content');
            mockIssuesGet.mockResolvedValue('issues content');

            const runConfig = {
                model: 'gpt-4',
                review: {
                    note: 'Test review note',
                    context: 'User context',
                    includeCommitHistory: true,
                    includeRecentDiffs: true,
                    includeReleaseNotes: true,
                    includeGithubIssues: true,
                    sendit: true
                }
            };

            await Review.execute(runConfig);

            const reviewNotesCall = mockFs.writeFile.mock.calls.find(
                (call: any) => call[0].includes('review-notes-') && !call[0].includes('.test')
            );
            expect(reviewNotesCall).toBeDefined();
            const content = reviewNotesCall![1];

            expect(content).toContain('# Review Notes\n\nTest review note');
            // Note: Context sections are now saved in the analysis result file, not in review notes
        });

        it('should handle file saving errors gracefully', async () => {
            const fs = await import('fs/promises');
            const mockFsWriteFile = fs.default.writeFile as any;

            // Mock writeFile to throw permission error for test files
            mockFsWriteFile.mockImplementation(async (path: string, data: any, encoding: any) => {
                if (path.includes('.test')) {
                    const error = new Error('Permission denied');
                    (error as any).code = 'EACCES';
                    throw error;
                }
                return;
            });

            const runConfig = {
                model: 'gpt-4',
                review: {
                    note: 'Test review note',
                    sendit: true
                }
            };

            const result = await Review.execute(runConfig);

            expect(mockLogger.warn).toHaveBeenCalledWith('Failed to save review notes: %s', expect.any(String));
            expect(mockLogger.warn).toHaveBeenCalledWith('Failed to save timestamped review analysis: %s', expect.any(String));
            expect(result).toBe('Issues created successfully');
        });

        it('should use custom output directory', async () => {
            const runConfig = {
                model: 'gpt-4',
                outputDirectory: 'custom-output',
                review: {
                    note: 'Test note',
                    sendit: true
                }
            };

            await Review.execute(runConfig);

            expect(mockStorage.ensureDirectory).toHaveBeenCalledWith('custom-output');

            // Get the actual fs module to check the calls
            const fs = await import('fs/promises');
            const mockFsWriteFile = fs.default.writeFile as any;

            // safeWriteFile creates test files first, then the actual files
            // The test files have .test extension and contain "test" content
            expect(mockFsWriteFile).toHaveBeenCalledWith(
                'custom-output/review-notes-123456.md.test',
                'test',
                'utf-8'
            );
            expect(mockFsWriteFile).toHaveBeenCalledWith(
                'custom-output/review-notes-123456.md',
                expect.any(String),
                'utf-8'
            );
            expect(mockFsWriteFile).toHaveBeenCalledWith(
                'custom-output/review-123456.md.test',
                'test',
                'utf-8'
            );
            expect(mockFsWriteFile).toHaveBeenCalledWith(
                'custom-output/review-123456.md',
                expect.any(String),
                'utf-8'
            );
        });
    });

    describe('GitHub issue creation', () => {
        it('should handle issue creation in sendit mode', async () => {
            const runConfig = {
                model: 'gpt-4',
                review: {
                    note: 'Test note',
                    sendit: true
                }
            };

            const result = await Review.execute(runConfig);

            expect(mockIssuesHandleIssueCreation).toHaveBeenCalledWith({
                summary: 'Review analysis summary',
                totalIssues: 3,
                issues: [
                    { title: 'Issue 1', priority: 'high' },
                    { title: 'Issue 2', priority: 'medium' },
                    { title: 'Issue 3', priority: 'low' }
                ]
            }, true);
            expect(result).toBe('Issues created successfully');
        });

        it('should handle issue creation in interactive mode', async () => {
            // Mock process.stdin.once for interactive mode
            const originalStdin = process.stdin;
            const mockStdin = {
                isTTY: true,
                once: vi.fn().mockImplementation((event, callback) => {
                    // Simulate user pressing Enter (approving the file)
                    setTimeout(() => callback(Buffer.from('\n')), 0);
                })
            };
            (process as any).stdin = mockStdin;

            try {
                const runConfig = {
                    model: 'gpt-4',
                    review: {
                        note: 'Test note',
                        sendit: false
                    }
                };

                const result = await Review.execute(runConfig);

                expect(mockIssuesHandleIssueCreation).toHaveBeenCalledWith({
                    summary: 'Review analysis summary',
                    totalIssues: 3,
                    issues: [
                        { title: 'Issue 1', priority: 'high' },
                        { title: 'Issue 2', priority: 'medium' },
                        { title: 'Issue 3', priority: 'low' }
                    ]
                }, false);
                expect(result).toBe('Issues created successfully');
            } finally {
                process.stdin = originalStdin;
            }
        });
    });

    describe('error handling', () => {
        it('should handle OpenAI API errors', async () => {
            mockCreateCompletion.mockRejectedValue(new Error('OpenAI API error'));

            const runConfig = {
                model: 'gpt-4',
                review: {
                    note: 'Test note',
                    sendit: true
                }
            };

            await expect(Review.execute(runConfig)).rejects.toThrow('Review analysis failed: OpenAI API error');
        });

        it('should handle prompt creation errors', async () => {
            mockCreatePrompt.mockRejectedValue(new Error('Prompt creation failed'));

            const runConfig = {
                model: 'gpt-4',
                review: {
                    note: 'Test note',
                    sendit: true
                }
            };

            await expect(Review.execute(runConfig)).rejects.toThrow('Prompt creation failed');
        });

        it('should handle storage directory creation errors', async () => {
            mockStorage.ensureDirectory.mockRejectedValue(new Error('Directory creation failed'));

            const runConfig = {
                model: 'gpt-4',
                review: {
                    note: 'Test note',
                    sendit: true
                }
            };

            await expect(Review.execute(runConfig)).rejects.toThrow('Directory creation failed');
        });
    });

    describe('complex scenarios', () => {
        it('should handle complete configuration with all features enabled', async () => {
            const runConfig = {
                model: 'gpt-4',
                debug: true,
                outputDirectory: 'custom-output',
                excludedPatterns: ['*.spec.ts'],
                discoveredConfigDirs: ['config1', 'config2'],
                overrides: true,
                review: {
                    note: 'Comprehensive review note',
                    context: 'Additional context',
                    includeCommitHistory: true,
                    includeRecentDiffs: true,
                    includeReleaseNotes: true,
                    includeGithubIssues: true,
                    commitHistoryLimit: 15,
                    diffHistoryLimit: 10,
                    releaseNotesLimit: 5,
                    githubIssuesLimit: 25,
                    sendit: true
                }
            };

            const result = await Review.execute(runConfig);

            expect(mockLogCreate).toHaveBeenCalledWith({ limit: 15 });
            expect(mockDiffGetRecentDiffsForReview).toHaveBeenCalledWith({
                limit: 10,
                baseExcludedPatterns: ['*.spec.ts']
            });
            expect(mockReleaseNotesGet).toHaveBeenCalledWith({ limit: 5 });
            expect(mockIssuesGet).toHaveBeenCalledWith({ limit: 25 });
            expect(mockStorage.ensureDirectory).toHaveBeenCalledWith('custom-output');
            expect(result).toBe('Issues created successfully');
        });

        it('should handle empty context content gracefully', async () => {
            mockLogGet.mockResolvedValue('');
            mockDiffGetRecentDiffsForReview.mockResolvedValue('');
            mockReleaseNotesGet.mockResolvedValue('');
            mockIssuesGet.mockResolvedValue('');

            const runConfig = {
                model: 'gpt-4',
                review: {
                    note: 'Test note',
                    includeCommitHistory: true,
                    includeRecentDiffs: true,
                    includeReleaseNotes: true,
                    includeGithubIssues: true,
                    sendit: true
                }
            };

            const result = await Review.execute(runConfig);

            expect(result).toBe('Issues created successfully');
            // The context gathering should complete successfully even with empty content
            expect(mockLogGet).toHaveBeenCalled();
            expect(mockDiffGetRecentDiffsForReview).toHaveBeenCalled();
            expect(mockReleaseNotesGet).toHaveBeenCalled();
            expect(mockIssuesGet).toHaveBeenCalled();
        });

        it('should handle analysis with no issues found', async () => {
            mockCreateCompletion.mockResolvedValue({
                summary: 'No issues found',
                totalIssues: 0,
                issues: []
            });

            const runConfig = {
                model: 'gpt-4',
                review: {
                    note: 'Test note',
                    sendit: true
                }
            };

            const result = await Review.execute(runConfig);

            expect(mockLogger.debug).toHaveBeenCalledWith('Total issues found: %d', 0);
            expect(mockLogger.debug).toHaveBeenCalledWith('Issues array length: %d', 0);
            expect(result).toBe('Issues created successfully');
        });

        it('should handle maxContextErrors configuration', async () => {
            // Mock multiple context gathering failures
            mockLogGet.mockRejectedValue(new Error('Log context failed'));
            mockDiffGetRecentDiffsForReview.mockRejectedValue(new Error('Diff context failed'));
            mockReleaseNotesGet.mockRejectedValue(new Error('Release notes context failed'));
            mockIssuesGet.mockRejectedValue(new Error('Issues context failed'));

            // Mock fs.readdir to return a file so that the directory processing logic is used
            mockFs.readdir.mockResolvedValue([
                { name: 'test-review.md', isFile: () => true }
            ]);

            // Mock fs.readFile to return valid content for the review file
            mockFs.readFile.mockResolvedValue('This is a valid review note content for testing');

            const runConfig = {
                model: 'gpt-4',
                review: {
                    directory: 'test-directory', // Use directory mode to trigger file processing
                    sendit: true,
                    includeCommitHistory: true,
                    includeRecentDiffs: true,
                    includeReleaseNotes: true,
                    includeGithubIssues: true,
                    maxContextErrors: 2 // Allow only 2 errors, but we'll have 4
                }
            };

            await expect(Review.execute(runConfig)).rejects.toThrow('Too many context gathering errors (4), aborting review. Consider checking your configuration and network connectivity.');
            expect(mockLogger.error).toHaveBeenCalledWith(
                expect.stringContaining('Too many context gathering errors (4), aborting review')
            );
        });

                 it('should handle custom editor timeout configuration', async () => {
             const runConfig = {
                 model: 'gpt-4',
                 review: {
                     note: 'Test note', // Provide note to avoid editor opening
                     editorTimeout: 60000, // 1 minute
                     sendit: true
                 }
             };

             const result = await Review.execute(runConfig);
             expect(result).toBe('Issues created successfully');
         });
    });

    describe('security and validation', () => {
        describe('createSecureTempFile function', () => {
            it('should handle temp directory access failures', async () => {
                const fs = await import('fs/promises');
                const mockFsAccess = fs.default.access as any;
                mockFsAccess.mockRejectedValue(new Error('Permission denied'));

                const runConfig = {
                    model: 'gpt-4',
                    review: {
                        sendit: true
                    }
                };

                await expect(Review.execute(runConfig)).rejects.toThrow('File operation failed on /tmp: Temp directory not writable: Permission denied');
            });

            it('should handle temp file creation failures', async () => {
                const fs = await import('fs/promises');
                const mockFsOpen = fs.default.open as any;
                mockFsOpen.mockRejectedValue(new Error('Cannot create file'));

                const runConfig = {
                    model: 'gpt-4',
                    review: {
                        sendit: true
                    }
                };

                await expect(Review.execute(runConfig)).rejects.toThrow('File operation failed on temporary file: Failed to create temp file: Cannot create file');
            });
        });

        describe('editor timeout handling', () => {
            it('should handle editor timeout gracefully', async () => {
                const childProcess = await import('child_process');
                const mockSpawn = childProcess.spawn as any;

                // Mock a child process that never exits (simulating timeout)
                const mockChild = {
                    on: vi.fn().mockImplementation((event, callback) => {
                        // Don't call any callbacks to simulate hanging
                    }),
                    kill: vi.fn()
                };
                mockSpawn.mockReturnValue(mockChild);

                const runConfig = {
                    model: 'gpt-4',
                    review: {
                        editorTimeout: 100, // Very short timeout for test
                        sendit: true
                    }
                };

                await expect(Review.execute(runConfig)).rejects.toThrow('Editor \'vi\' timed out after 100ms. Consider using a different editor or increasing the timeout.');
            });

            it('should handle editor SIGTERM signal', async () => {
                const childProcess = await import('child_process');
                const mockSpawn = childProcess.spawn as any;

                const mockChild = {
                    on: vi.fn().mockImplementation((event, callback) => {
                        if (event === 'exit') {
                            setTimeout(() => callback(0, 'SIGTERM'), 0);
                        }
                    }),
                    kill: vi.fn()
                };
                mockSpawn.mockReturnValue(mockChild);

                const runConfig = {
                    model: 'gpt-4',
                    review: {
                        sendit: true
                    }
                };

                await expect(Review.execute(runConfig)).rejects.toThrow('Editor was terminated (SIGTERM)');
            });

            it('should handle editor non-zero exit code', async () => {
                const childProcess = await import('child_process');
                const mockSpawn = childProcess.spawn as any;

                const mockChild = {
                    on: vi.fn().mockImplementation((event, callback) => {
                        if (event === 'exit') {
                            setTimeout(() => callback(1), 0); // Non-zero exit code
                        }
                    }),
                    kill: vi.fn()
                };
                mockSpawn.mockReturnValue(mockChild);

                const runConfig = {
                    model: 'gpt-4',
                    review: {
                        sendit: true
                    }
                };

                await expect(Review.execute(runConfig)).rejects.toThrow('Editor exited with non-zero code: 1');
            });

            it('should handle editor without timeout when editorTimeout is not specified', async () => {
                const runConfig = {
                    model: 'gpt-4',
                    review: {
                        note: 'Test review note without timeout', // Provide note to avoid editor opening
                        // No editorTimeout specified - should work without timeout
                        sendit: true,
                        editorTimeout: undefined
                    }
                };

                const result = await Review.execute(runConfig);
                expect(result).toBe('Issues created successfully');

                // Test that editorTimeout was correctly passed through as undefined
                expect(runConfig.review.editorTimeout).toBeUndefined();
            });
        });

        describe('validateReviewResult function', () => {
            it('should reject null or undefined responses', async () => {
                mockCreateCompletion.mockResolvedValue(null);

                const runConfig = {
                    model: 'gpt-4',
                    review: {
                        note: 'Test note',
                        sendit: true
                    }
                };

                await expect(Review.execute(runConfig)).rejects.toThrow('Review analysis failed: Invalid API response: expected object, got object');
                expect(mockLogger.error).toHaveBeenCalledWith(
                    expect.stringContaining('Invalid API response: expected object, got object')
                );
            });

            it('should reject responses with invalid summary', async () => {
                mockCreateCompletion.mockResolvedValue({
                    summary: 123, // Should be string
                    totalIssues: 0,
                    issues: []
                });

                const runConfig = {
                    model: 'gpt-4',
                    review: {
                        note: 'Test note',
                        sendit: true
                    }
                };

                await expect(Review.execute(runConfig)).rejects.toThrow('Review analysis failed: Invalid API response: missing or invalid summary field');
                expect(mockLogger.error).toHaveBeenCalledWith(
                    expect.stringContaining('Invalid API response: missing or invalid summary field')
                );
            });

            it('should reject responses with invalid totalIssues', async () => {
                mockCreateCompletion.mockResolvedValue({
                    summary: 'Valid summary',
                    totalIssues: -1, // Should be non-negative
                    issues: []
                });

                const runConfig = {
                    model: 'gpt-4',
                    review: {
                        note: 'Test note',
                        sendit: true
                    }
                };

                await expect(Review.execute(runConfig)).rejects.toThrow('Review analysis failed: Invalid API response: missing or invalid totalIssues field');
                expect(mockLogger.error).toHaveBeenCalledWith(
                    expect.stringContaining('Invalid API response: missing or invalid totalIssues field')
                );
            });

            it('should reject responses with non-array issues', async () => {
                mockCreateCompletion.mockResolvedValue({
                    summary: 'Valid summary',
                    totalIssues: 1,
                    issues: 'not an array'
                });

                const runConfig = {
                    model: 'gpt-4',
                    review: {
                        note: 'Test note',
                        sendit: true
                    }
                };

                await expect(Review.execute(runConfig)).rejects.toThrow('Review analysis failed: Invalid API response: issues field must be an array');
                expect(mockLogger.error).toHaveBeenCalledWith(
                    expect.stringContaining('Invalid API response: issues field must be an array')
                );
            });

            it('should reject responses with invalid issue objects', async () => {
                mockCreateCompletion.mockResolvedValue({
                    summary: 'Valid summary',
                    totalIssues: 2,
                    issues: [
                        { title: 'Valid issue', priority: 'high' },
                        { title: 'Invalid issue' } // Missing priority
                    ]
                });

                const runConfig = {
                    model: 'gpt-4',
                    review: {
                        note: 'Test note',
                        sendit: true
                    }
                };

                await expect(Review.execute(runConfig)).rejects.toThrow('Review analysis failed: Invalid API response: issue 1 missing priority');
                expect(mockLogger.error).toHaveBeenCalledWith(
                    expect.stringContaining('Invalid API response: issue 1 missing priority')
                );
            });
        });

        describe('safeWriteFile function', () => {
            it('should handle parent directory access failures', async () => {
                const fs = await import('fs/promises');
                const mockFsAccess = fs.default.access as any;

                // Allow temp file access but fail on output directory
                mockFsAccess.mockImplementation(async (path: string) => {
                    if (path.includes('kodrdriv_review_')) {
                        return; // Allow temp files
                    }
                    if (path.includes('output') || path.includes('custom-output')) {
                        throw new Error('Directory not writable');
                    }
                    return;
                });

                const runConfig = {
                    model: 'gpt-4',
                    outputDirectory: 'custom-output',
                    review: {
                        note: 'Test note',
                        sendit: true
                    }
                };

                // Should still succeed but log warnings about file saves
                const result = await Review.execute(runConfig);
                expect(result).toBe('Issues created successfully');
                expect(mockLogger.warn).toHaveBeenCalledWith(
                    'Failed to save review notes: %s',
                    expect.any(String)
                );
            });

            it('should handle disk space errors (ENOSPC)', async () => {
                const fs = await import('fs/promises');
                const mockFsWriteFile = fs.default.writeFile as any;

                // Mock ENOSPC error for test files
                mockFsWriteFile.mockImplementation(async (path: string, data: any, encoding: any) => {
                    if (path.includes('.test')) {
                        const error = new Error('No space left on device');
                        (error as any).code = 'ENOSPC';
                        throw error;
                    }
                    return;
                });

                const runConfig = {
                    model: 'gpt-4',
                    review: {
                        note: 'Test note',
                        sendit: true
                    }
                };

                // Should still succeed but log warnings
                const result = await Review.execute(runConfig);
                expect(result).toBe('Issues created successfully');
                expect(mockLogger.warn).toHaveBeenCalledWith(
                    'Failed to save review notes: %s',
                    expect.any(String)
                );
            });
        });

        describe('TTY detection edge cases', () => {
            it('should handle undefined TTY with fallback checks', async () => {
                // Mock undefined TTY scenario
                const originalStdin = process.stdin;
                const originalStdout = process.stdout;
                const originalStderr = process.stderr;

                try {
                    (process as any).stdin = { isTTY: undefined };
                    (process as any).stdout = { isTTY: true };
                    (process as any).stderr = { isTTY: true };

                    const runConfig = {
                        model: 'gpt-4',
                        review: {
                            note: 'Test note',
                            sendit: true
                        }
                    };

                    const result = await Review.execute(runConfig);
                    expect(result).toBe('Issues created successfully');
                } finally {
                    process.stdin = originalStdin;
                    process.stdout = originalStdout;
                    process.stderr = originalStderr;
                }
            });

                         it('should handle TTY detection exceptions', async () => {
                 // Test the scenario where TTY detection would handle exceptions
                 // Since we can't easily mock process.stdin throwing, we'll test
                 // that the command works with sendit mode regardless of TTY state
                 const runConfig = {
                     model: 'gpt-4',
                     review: {
                         note: 'Test note',
                         sendit: true // Should work with sendit even if TTY detection fails
                     }
                 };

                 const result = await Review.execute(runConfig);
                 expect(result).toBe('Issues created successfully');
                 // The function should work regardless of TTY detection issues
             });
        });
    });

    describe('edge cases and error scenarios', () => {
                 it('should handle cleanup errors for non-existent files', async () => {
             const fs = await import('fs/promises');
             const mockFsUnlink = fs.default.unlink as any;

             // Mock ENOENT error (file not found) for cleanup only
             mockFsUnlink.mockImplementation(async (path: string) => {
                 if (path.includes('kodrdriv_review_')) {
                     // Simulate cleanup error for temp files
                     const error = new Error('File not found');
                     (error as any).code = 'ENOENT';
                     throw error;
                 }
                 return; // Allow other file operations
             });

             const runConfig = {
                 model: 'gpt-4',
                 review: {
                     note: 'Test note', // Provide note to avoid editor issues
                     sendit: true
                 }
             };

             // Should not throw despite cleanup error
             const result = await Review.execute(runConfig);
             expect(result).toBe('Issues created successfully');
             // ENOENT errors should be silently ignored in cleanup
             expect(mockLogger.warn).not.toHaveBeenCalledWith(
                 expect.stringContaining('Failed to cleanup temp file')
             );
         });

                                            it('should handle file system errors gracefully during execution', async () => {
             // Test that file system errors don't crash the entire operation
             // Mock access to throw for test files only to simulate disk space/permission issues
             const fs = await import('fs/promises');
             const mockFsAccess = fs.default.access as any;

             mockFsAccess.mockImplementation(async (path: string) => {
                 if (path.includes('.test')) {
                     throw new Error('Disk space insufficient');
                 }
                 return; // Allow other file operations
             });

             const runConfig = {
                 model: 'gpt-4',
                 review: {
                     note: 'Test note with sufficient content',
                     sendit: true
                 }
             };

             // Should complete successfully despite file system errors for test files
             const result = await Review.execute(runConfig);
             expect(result).toBe('Issues created successfully');
         });

         it('should validate review note content properly', async () => {
             // Test direct validation without complex editor mocking
             const runConfig = {
                 model: 'gpt-4',
                 review: {
                     note: 'Valid review note with adequate content for analysis',
                     sendit: true
                 }
             };

             const result = await Review.execute(runConfig);
             expect(result).toBe('Issues created successfully');

             // Verify the note was processed
             expect(mockLogger.debug).toHaveBeenCalledWith(
                 'Review note: %s',
                 'Valid review note with adequate content for analysis'
             );
         });

        it('should handle mixed success and failure in context gathering', async () => {
            // Mix of success and failure
            mockLogGet.mockResolvedValue('successful log content');
            mockDiffGetRecentDiffsForReview.mockRejectedValue(new Error('Diff failed'));
            mockReleaseNotesGet.mockResolvedValue('successful release notes');
            mockIssuesGet.mockRejectedValue(new Error('Issues failed'));

            const runConfig = {
                model: 'gpt-4',
                review: {
                    note: 'Test note',
                    includeCommitHistory: true,
                    includeRecentDiffs: true,
                    includeReleaseNotes: true,
                    includeGithubIssues: true,
                    sendit: true
                }
            };

            const result = await Review.execute(runConfig);

            expect(result).toBe('Issues created successfully');
            expect(mockLogger.warn).toHaveBeenCalledWith('Context gathering completed with 2 error(s):');
            expect(mockLogger.warn).toHaveBeenCalledWith('  - Failed to fetch recent diffs: Diff failed');
            expect(mockLogger.warn).toHaveBeenCalledWith('  - Failed to fetch GitHub issues: Issues failed');
        });

        it('should handle very long review notes', async () => {
            const longNote = 'A'.repeat(100000); // 100KB note

            const runConfig = {
                model: 'gpt-4',
                review: {
                    note: longNote,
                    sendit: true
                }
            };

            const result = await Review.execute(runConfig);
            expect(result).toBe('Issues created successfully');
            expect(mockLogger.debug).toHaveBeenCalledWith('Review note length: %d characters', 100000);
        });

                                                     it('should handle environment variable configurations', async () => {
             // Test that the command respects environment configurations
             mockProcess.env.CUSTOM_VAR = 'test-value';

             const runConfig = {
                 model: 'gpt-4',
                 review: {
                     note: 'Test note with environment context',
                     sendit: true
                 }
             };

             const result = await Review.execute(runConfig);
             expect(result).toBe('Issues created successfully');

             // Verify that the configuration was processed
             expect(mockLogger.debug).toHaveBeenCalledWith(
                 'Review note: %s',
                 'Test note with environment context'
             );
         });

        it('should handle prompt creation with empty discovered config dirs', async () => {
            const runConfig = {
                model: 'gpt-4',
                discoveredConfigDirs: [], // Explicitly empty
                overrides: false,
                review: {
                    note: 'Test note',
                    sendit: true
                }
            };

            const result = await Review.execute(runConfig);

            expect(mockCreatePrompt).toHaveBeenCalledWith(
                expect.objectContaining({
                    overridePaths: [],
                    overrides: false
                }),
                expect.any(Object),
                expect.any(Object)
            );
            expect(result).toBe('Issues created successfully');
        });
    });

    describe('directory processing', () => {
        it('should process files from directory when directory is specified', async () => {
            // Mock fs.readdir to return multiple files
            mockFs.readdir.mockResolvedValue([
                { name: 'review1.md', isFile: () => true },
                { name: 'review2.md', isFile: () => true },
                { name: 'review3.md', isFile: () => true }
            ]);

            const runConfig = {
                model: 'gpt-4',
                review: {
                    directory: 'test-reviews',
                    sendit: true
                }
            };

            const result = await Review.execute(runConfig);

            expect(mockLogger.info).toHaveBeenCalledWith('📁 Processing review files in directory: test-reviews');
            expect(mockLogger.info).toHaveBeenCalledWith('📁 Found 3 files to process');
            expect(mockLogger.info).toHaveBeenCalledWith('Auto-selecting all 3 files for processing (--sendit mode)');
            expect(result).toBe('Issues created successfully');
        });

        it('should throw error when directory contains no files', async () => {
            mockFs.readdir.mockResolvedValue([]);

            const runConfig = {
                model: 'gpt-4',
                review: {
                    directory: 'empty-directory',
                    sendit: true
                }
            };

            await expect(Review.execute(runConfig)).rejects.toThrow('No review files found in directory: empty-directory');
        });

        it('should throw error when directory does not exist', async () => {
            const fs = await import('fs/promises');
            const mockFsReaddir = fs.default.readdir as any;
            mockFsReaddir.mockRejectedValue({ code: 'ENOENT', message: 'Directory not found' });

            const runConfig = {
                model: 'gpt-4',
                review: {
                    directory: 'non-existent-directory',
                    sendit: true
                }
            };

            await expect(Review.execute(runConfig)).rejects.toThrow('Directory not found: non-existent-directory');
        });

        it('should handle directory read errors gracefully', async () => {
            const fs = await import('fs/promises');
            const mockFsReaddir = fs.default.readdir as any;
            mockFsReaddir.mockRejectedValue({ code: 'EACCES', message: 'Permission denied' });

            const runConfig = {
                model: 'gpt-4',
                review: {
                    directory: 'permission-denied-directory',
                    sendit: true
                }
            };

            await expect(Review.execute(runConfig)).rejects.toThrow('Failed to read directory: permission-denied-directory');
        });

        it('should filter out directories and only process files', async () => {
            mockFs.readdir.mockResolvedValue([
                { name: 'review1.md', isFile: () => true },
                { name: 'subdirectory', isFile: () => false },
                { name: 'review2.md', isFile: () => true }
            ]);

            const runConfig = {
                model: 'gpt-4',
                review: {
                    directory: 'mixed-directory',
                    sendit: true
                }
            };

            const result = await Review.execute(runConfig);

            expect(mockLogger.info).toHaveBeenCalledWith('📁 Found 2 files to process');
            expect(result).toBe('Issues created successfully');
        });

        it('should sort files alphabetically', async () => {
            mockFs.readdir.mockResolvedValue([
                { name: 'zebra.md', isFile: () => true },
                { name: 'alpha.md', isFile: () => true },
                { name: 'beta.md', isFile: () => true }
            ]);

            const runConfig = {
                model: 'gpt-4',
                review: {
                    directory: 'unsorted-directory',
                    sendit: true
                }
            };

            await Review.execute(runConfig);

            // The files should be processed in alphabetical order
            expect(mockLogger.info).toHaveBeenCalledWith('📁 Found 3 files to process');
        });
    });

    describe('file selection and processing', () => {
        it('should handle file selection in interactive mode', async () => {
            mockFs.readdir.mockResolvedValue([
                { name: 'file1.md', isFile: () => true },
                { name: 'file2.md', isFile: () => true }
            ]);

            // Mock getUserChoice to return different choices
            mockGetUserChoice
                .mockResolvedValueOnce('c') // Confirm first file
                .mockResolvedValueOnce('s'); // Skip second file

            const runConfig = {
                model: 'gpt-4',
                review: {
                    directory: 'test-directory',
                    sendit: false
                }
            };

            const result = await Review.execute(runConfig);

            expect(mockLogger.info).toHaveBeenCalledWith('\n📁 File Selection Phase');
            expect(mockLogger.info).toHaveBeenCalledWith('Found 2 files to review. Select which ones to process:');
            expect(mockLogger.info).toHaveBeenCalledWith('✅ File selected for processing: test-directory/file1.md');
            expect(mockLogger.info).toHaveBeenCalledWith('⏭️  File skipped: test-directory/file2.md');
            expect(result).toBe('Issues created successfully');
        });

        it('should handle abort during file selection', async () => {
            mockFs.readdir.mockResolvedValue([
                { name: 'file1.md', isFile: () => true },
                { name: 'file2.md', isFile: () => true }
            ]);

            mockGetUserChoice.mockResolvedValueOnce('a'); // Abort

            const runConfig = {
                model: 'gpt-4',
                review: {
                    directory: 'test-directory',
                    sendit: false
                }
            };

            await expect(Review.execute(runConfig)).rejects.toThrow('Review process aborted by user');
            expect(mockLogger.info).toHaveBeenCalledWith('🛑 Aborting review process as requested');
        });

        it('should handle no files selected during file selection', async () => {
            mockFs.readdir.mockResolvedValue([
                { name: 'file1.md', isFile: () => true },
                { name: 'file2.md', isFile: () => true }
            ]);

            mockGetUserChoice
                .mockResolvedValueOnce('s') // Skip first file
                .mockResolvedValueOnce('s'); // Skip second file

            const runConfig = {
                model: 'gpt-4',
                review: {
                    directory: 'test-directory',
                    sendit: false
                }
            };

            await expect(Review.execute(runConfig)).rejects.toThrow('No files were selected for processing');
        });

        it('should handle non-interactive environment in file selection', async () => {
            mockProcess.stdin.isTTY = false;
            mockFs.readdir.mockResolvedValue([
                { name: 'file1.md', isFile: () => true },
                { name: 'file2.md', isFile: () => true }
            ]);

            const runConfig = {
                model: 'gpt-4',
                review: {
                    directory: 'test-directory',
                    sendit: true // Need sendit for non-interactive
                }
            };

            const result = await Review.execute(runConfig);

            // In sendit mode, the warning about non-interactive environment is not shown
            // because sendit mode auto-selects all files
            expect(result).toBe('Issues created successfully');
        });

        it('should handle non-interactive environment without sendit', async () => {
            mockProcess.stdin.isTTY = false;
            mockFs.readdir.mockResolvedValue([
                { name: 'file1.md', isFile: () => true }
            ]);

            const runConfig = {
                model: 'gpt-4',
                review: {
                    directory: 'test-directory',
                    sendit: false
                }
            };

            await expect(Review.execute(runConfig)).rejects.toThrow('Piped input requires --sendit flag for non-interactive operation');
        });
    });

    describe('multiple file processing', () => {
        it('should process multiple files and combine results', async () => {
            mockFs.readdir.mockResolvedValue([
                { name: 'file1.md', isFile: () => true },
                { name: 'file2.md', isFile: () => true }
            ]);

            // Mock different results for each file
            mockCreateCompletion
                .mockResolvedValueOnce({
                    summary: 'First file analysis',
                    totalIssues: 2,
                    issues: [
                        { title: 'Issue 1', priority: 'high' },
                        { title: 'Issue 2', priority: 'medium' }
                    ]
                })
                .mockResolvedValueOnce({
                    summary: 'Second file analysis',
                    totalIssues: 1,
                    issues: [
                        { title: 'Issue 3', priority: 'low' }
                    ]
                });

            const runConfig = {
                model: 'gpt-4',
                review: {
                    directory: 'test-directory',
                    sendit: true
                }
            };

            const result = await Review.execute(runConfig);

            expect(mockLogger.info).toHaveBeenCalledWith('✅ Successfully processed 2 review files');
            expect(mockLogger.info).toHaveBeenCalledWith('📝 Processing file 1/2: test-directory/file1.md');
            expect(mockLogger.info).toHaveBeenCalledWith('📝 Processing file 2/2: test-directory/file2.md');
            expect(result).toBe('Issues created successfully');

            // Verify combined results were saved
            const fs = await import('fs/promises');
            const mockFsWriteFile = fs.default.writeFile as any;
            expect(mockFsWriteFile).toHaveBeenCalledWith(
                'output/review-123456.md',
                expect.stringContaining('# Combined Review Analysis Result'),
                'utf-8'
            );
        });

        it('should handle file processing errors gracefully in directory mode', async () => {
            mockFs.readdir.mockResolvedValue([
                { name: 'file1.md', isFile: () => true },
                { name: 'file2.md', isFile: () => true }
            ]);

            // Mock first file to succeed, second to fail
            mockCreateCompletion
                .mockResolvedValueOnce({
                    summary: 'First file analysis',
                    totalIssues: 1,
                    issues: [{ title: 'Issue 1', priority: 'high' }]
                })
                .mockRejectedValueOnce(new Error('Processing failed'));

            const runConfig = {
                model: 'gpt-4',
                review: {
                    directory: 'test-directory',
                    sendit: true
                }
            };

            const result = await Review.execute(runConfig);

            expect(mockLogger.warn).toHaveBeenCalledWith('Failed to process file test-directory/file2.md: Review analysis failed: Processing failed');
            expect(result).toBe('Issues created successfully');
        });

        it('should throw error when no files are processed successfully', async () => {
            mockFs.readdir.mockResolvedValue([
                { name: 'file1.md', isFile: () => true },
                { name: 'file2.md', isFile: () => true }
            ]);

            // Mock both files to fail
            mockCreateCompletion
                .mockRejectedValueOnce(new Error('First file failed'))
                .mockRejectedValueOnce(new Error('Second file failed'));

            const runConfig = {
                model: 'gpt-4',
                review: {
                    directory: 'test-directory',
                    sendit: true
                }
            };

            await expect(Review.execute(runConfig)).rejects.toThrow('No files were processed successfully');
        });



        it('should handle non-critical errors gracefully in directory mode', async () => {
            mockFs.readdir.mockResolvedValue([
                { name: 'file1.md', isFile: () => true }
            ]);

            // Mock a non-critical error that should be caught and logged
            mockCreateCompletion.mockRejectedValue(new Error('Some other error'));

            const runConfig = {
                model: 'gpt-4',
                review: {
                    directory: 'test-directory',
                    sendit: true
                }
            };

            // This should fail because no files were processed successfully
            await expect(Review.execute(runConfig)).rejects.toThrow('No files were processed successfully');
        });
    });

    describe('file reading functionality', () => {
        it('should read review note from file successfully', async () => {
            mockFs.readFile.mockResolvedValue('Test review content from file');

            const runConfig = {
                model: 'gpt-4',
                review: {
                    file: 'test-review.md',
                    sendit: true
                }
            };

            const result = await Review.execute(runConfig);

            expect(mockLogger.info).toHaveBeenCalledWith('📁 Reading review note from file: test-review.md');
            expect(mockLogger.debug).toHaveBeenCalledWith('Successfully read review note from file: test-review.md (29 characters)');
            expect(result).toBe('Issues created successfully');
        });

        it('should throw error when review file is empty', async () => {
            mockFs.readFile.mockResolvedValue('   \n\n  ');

            const runConfig = {
                model: 'gpt-4',
                review: {
                    file: 'empty-review.md',
                    sendit: true
                }
            };

            await expect(Review.execute(runConfig)).rejects.toThrow('Review file is empty: empty-review.md');
        });

        it('should throw error when review file does not exist', async () => {
            const fs = await import('fs/promises');
            const mockFsReadFile = fs.default.readFile as any;
            mockFsReadFile.mockRejectedValue({ code: 'ENOENT', message: 'File not found' });

            const runConfig = {
                model: 'gpt-4',
                review: {
                    file: 'non-existent-review.md',
                    sendit: true
                }
            };

            await expect(Review.execute(runConfig)).rejects.toThrow('Review file not found: non-existent-review.md');
        });

        it('should handle file read errors gracefully', async () => {
            const fs = await import('fs/promises');
            const mockFsReadFile = fs.default.readFile as any;
            mockFsReadFile.mockRejectedValue({ code: 'EACCES', message: 'Permission denied' });

            const runConfig = {
                model: 'gpt-4',
                review: {
                    file: 'permission-denied-review.md',
                    sendit: true
                }
            };

            await expect(Review.execute(runConfig)).rejects.toThrow('Failed to read review file: Permission denied');
        });
    });

    describe('dry run mode enhancements', () => {
        it('should show directory processing in dry run mode', async () => {
            const runConfig = {
                model: 'gpt-4',
                dryRun: true,
                review: {
                    directory: 'test-directory'
                }
            };

            const result = await Review.execute(runConfig);

            expect(mockLogger.info).toHaveBeenCalledWith('DRY RUN: Would process review files in directory: %s', 'test-directory');
            expect(mockLogger.info).toHaveBeenCalledWith('DRY RUN: Would first select which files to process, then analyze selected files');
            expect(result).toBe('DRY RUN: Review command would analyze note, gather context, and create GitHub issues');
        });

        it('should show file reading in dry run mode', async () => {
            const runConfig = {
                model: 'gpt-4',
                dryRun: true,
                review: {
                    file: 'test-review.md'
                }
            };

            const result = await Review.execute(runConfig);

            expect(mockLogger.info).toHaveBeenCalledWith('DRY RUN: Would read review note from file: %s', 'test-review.md');
            expect(result).toBe('DRY RUN: Review command would analyze note, gather context, and create GitHub issues');
        });

        it('should show editor opening in dry run mode', async () => {
            const runConfig = {
                model: 'gpt-4',
                dryRun: true,
                review: {}
            };

            const result = await Review.execute(runConfig);

            expect(mockLogger.info).toHaveBeenCalledWith('DRY RUN: Would open editor to capture review note');
            expect(result).toBe('DRY RUN: Review command would analyze note, gather context, and create GitHub issues');
        });
    });

    describe('error handling enhancements', () => {
        it('should handle ValidationError in execute wrapper', async () => {
            // Mock fs.readdir to return empty array to trigger validation error
            mockFs.readdir.mockResolvedValue([]);

            const runConfig = {
                model: 'gpt-4',
                review: {
                    directory: 'empty-directory',
                    sendit: true
                }
            };

            await expect(Review.execute(runConfig)).rejects.toThrow('No review files found in directory: empty-directory');
            expect(mockLogger.error).toHaveBeenCalledWith('review failed: No review files found in directory: empty-directory');
        });

        it('should handle FileOperationError in execute wrapper', async () => {
            const fs = await import('fs/promises');
            const mockFsReadFile = fs.default.readFile as any;
            mockFsReadFile.mockRejectedValue({ code: 'ENOENT', message: 'File not found' });

            const runConfig = {
                model: 'gpt-4',
                review: {
                    file: 'non-existent.md'
                }
            };

            await expect(Review.execute(runConfig)).rejects.toThrow('Review file not found: non-existent.md');
            expect(mockLogger.error).toHaveBeenCalledWith('review failed: File operation failed on non-existent.md: Review file not found: non-existent.md');
        });

        it('should handle CommandError in execute wrapper', async () => {
            // Mock a CommandError to be thrown
            const { CommandError } = await import('../../src/error/CommandErrors');
            mockCreateCompletion.mockRejectedValue(new CommandError('Test command error', 'TEST_ERROR'));

            const runConfig = {
                model: 'gpt-4',
                review: {
                    note: 'Test note',
                    sendit: true
                }
            };

            await expect(Review.execute(runConfig)).rejects.toThrow('Test command error');
            expect(mockLogger.error).toHaveBeenCalledWith('review encountered unexpected error: Review analysis failed: Test command error');
        });

        it('should handle unexpected errors in execute wrapper', async () => {
            mockCreateCompletion.mockRejectedValue(new Error('Unexpected error'));

            const runConfig = {
                model: 'gpt-4',
                review: {
                    note: 'Test note',
                    sendit: true
                }
            };

            await expect(Review.execute(runConfig)).rejects.toThrow('Unexpected error');
            expect(mockLogger.error).toHaveBeenCalledWith('review encountered unexpected error: Review analysis failed: Unexpected error');
        });
    });

    describe('context gathering enhancements', () => {
        it('should handle empty context content gracefully', async () => {
            mockLogGet.mockResolvedValue('');
            mockDiffGetRecentDiffsForReview.mockResolvedValue('');
            mockReleaseNotesGet.mockResolvedValue('');
            mockIssuesGet.mockResolvedValue('');

            const runConfig = {
                model: 'gpt-4',
                review: {
                    note: 'Test note',
                    includeCommitHistory: true,
                    includeRecentDiffs: true,
                    includeReleaseNotes: true,
                    includeGithubIssues: true,
                    sendit: true
                }
            };

            const result = await Review.execute(runConfig);

            expect(result).toBe('Issues created successfully');
            // The context gathering should complete successfully even with empty content
            expect(mockLogGet).toHaveBeenCalled();
            expect(mockDiffGetRecentDiffsForReview).toHaveBeenCalled();
            expect(mockReleaseNotesGet).toHaveBeenCalled();
            expect(mockIssuesGet).toHaveBeenCalled();
        });

        it('should handle context gathering with custom limits', async () => {
            const runConfig = {
                model: 'gpt-4',
                review: {
                    note: 'Test note',
                    includeCommitHistory: true,
                    includeRecentDiffs: true,
                    includeReleaseNotes: true,
                    includeGithubIssues: true,
                    commitHistoryLimit: 25,
                    diffHistoryLimit: 15,
                    releaseNotesLimit: 8,
                    githubIssuesLimit: 30,
                    sendit: true
                }
            };

            await Review.execute(runConfig);

            expect(mockLogCreate).toHaveBeenCalledWith({ limit: 25 });
            expect(mockDiffGetRecentDiffsForReview).toHaveBeenCalledWith({
                limit: 15,
                baseExcludedPatterns: ['node_modules', '*.test.ts']
            });
            expect(mockReleaseNotesGet).toHaveBeenCalledWith({ limit: 8 });
            expect(mockIssuesGet).toHaveBeenCalledWith({ limit: 30 });
        });
    });

    describe('file saving enhancements', () => {
        it('should handle combined results file saving errors', async () => {
            mockFs.readdir.mockResolvedValue([
                { name: 'file1.md', isFile: () => true },
                { name: 'file2.md', isFile: () => true }
            ]);

            // Mock file system to fail on combined results save
            const fs = await import('fs/promises');
            const mockFsWriteFile = fs.default.writeFile as any;
            mockFsWriteFile.mockImplementation(async (path: string, content: string) => {
                if (path.includes('review-123456.md') && !path.includes('.test')) {
                    throw new Error('Failed to save combined results');
                }
                return;
            });

            const runConfig = {
                model: 'gpt-4',
                review: {
                    directory: 'test-directory',
                    sendit: true
                }
            };

            const result = await Review.execute(runConfig);

            expect(mockLogger.warn).toHaveBeenCalledWith('Failed to save combined review analysis: %s', 'Failed to write file output/review-123456.md: Failed to save combined results');
            expect(result).toBe('Issues created successfully');
        });

        it('should handle review notes file saving errors', async () => {
            const fs = await import('fs/promises');
            const mockFsWriteFile = fs.default.writeFile as any;
            mockFsWriteFile.mockImplementation(async (path: string, content: string) => {
                if (path.includes('review-notes-123456.md') && !path.includes('.test')) {
                    throw new Error('Failed to save review notes');
                }
                return;
            });

            const runConfig = {
                model: 'gpt-4',
                review: {
                    note: 'Test note',
                    sendit: true
                }
            };

            const result = await Review.execute(runConfig);

            expect(mockLogger.warn).toHaveBeenCalledWith('Failed to save review notes: %s', 'Failed to write file output/review-notes-123456.md: Failed to save review notes');
            expect(result).toBe('Issues created successfully');
        });
    });

    describe('interactive mode enhancements', () => {
        it('should handle interactive mode with file processing confirmation', async () => {
            mockFs.readdir.mockResolvedValue([
                { name: 'file1.md', isFile: () => true }
            ]);

            // Mock process.stdin.once for interactive confirmation
            const originalStdin = process.stdin;
            const mockStdin = {
                isTTY: true,
                once: vi.fn().mockImplementation((event, callback) => {
                    // Simulate user pressing Enter (confirming the file)
                    setTimeout(() => callback(Buffer.from('\n')), 0);
                })
            };
            (process as any).stdin = mockStdin;

            try {
                const runConfig = {
                    model: 'gpt-4',
                    review: {
                        directory: 'test-directory',
                        sendit: false
                    }
                };

                const result = await Review.execute(runConfig);

                expect(result).toBe('Issues created successfully');
            } finally {
                process.stdin = originalStdin;
            }
        });

        it('should handle interactive mode with file skip', async () => {
            mockFs.readdir.mockResolvedValue([
                { name: 'file1.md', isFile: () => true }
            ]);

            // Mock getUserChoice to return 's' for skip
            mockGetUserChoice.mockResolvedValueOnce('s');

            const runConfig = {
                model: 'gpt-4',
                review: {
                    directory: 'test-directory',
                    sendit: false
                }
            };

            await expect(Review.execute(runConfig)).rejects.toThrow('No files were selected for processing');
        });
    });
});
