import { describe, it, beforeAll, beforeEach, afterEach, expect, vi } from 'vitest';

// Simplified mock setup to prevent memory leaks
vi.mock('../../src/prompt/release', () => ({
    createPrompt: vi.fn().mockResolvedValue({
        prompt: 'mock prompt',
        isLargeRelease: false,
        maxTokens: 4000
    })
}));

vi.mock('../../src/content/log', () => ({
    create: vi.fn().mockReturnValue({
        get: vi.fn().mockResolvedValue('mock log content')
    })
}));

vi.mock('../../src/content/diff', () => ({
    create: vi.fn().mockReturnValue({
        get: vi.fn().mockResolvedValue('mock diff content')
    }),
    truncateDiffByFiles: vi.fn().mockImplementation((content, maxBytes) => content.substring(0, maxBytes))
}));

vi.mock('../../src/util/openai', () => ({
    createCompletionWithRetry: vi.fn().mockResolvedValue({
        title: 'mock title',
        body: 'mock body'
    }),
    getModelForCommand: vi.fn().mockReturnValue('gpt-4o-mini'),
    getOpenAIReasoningForCommand: vi.fn().mockReturnValue('low'),
    getOpenAIMaxOutputTokensForCommand: vi.fn().mockReturnValue(10000)
}));

vi.mock('../../src/logging', () => ({
    getLogger: vi.fn().mockReturnValue({
        info: vi.fn(),
        debug: vi.fn(),
        warn: vi.fn(),
        error: vi.fn(),
        verbose: vi.fn(),
        silly: vi.fn()
    }),
    getDryRunLogger: vi.fn().mockReturnValue({
        info: vi.fn(),
        debug: vi.fn(),
        warn: vi.fn(),
        error: vi.fn(),
        verbose: vi.fn(),
        silly: vi.fn()
    })
}));

vi.mock('../../src/util/validation', () => ({
    validateReleaseSummary: vi.fn().mockImplementation((data) => data),
    safeJsonParse: vi.fn().mockImplementation((text: string) => JSON.parse(text))
}));

vi.mock('../../src/util/github', () => ({
    getMilestoneIssuesForRelease: vi.fn().mockResolvedValue('')
}));

vi.mock('../../src/util/general', () => ({
    getOutputPath: vi.fn().mockImplementation((dir, file) => `${dir}/${file}`),
    getTimestampedRequestFilename: vi.fn().mockReturnValue('request-123456.json'),
    getTimestampedResponseFilename: vi.fn().mockReturnValue('response-123456.json'),
    getTimestampedReleaseNotesFilename: vi.fn().mockReturnValue('release-notes-123456.md')
}));

vi.mock('../../src/util/storage', () => ({
    create: vi.fn().mockReturnValue({
        ensureDirectory: vi.fn().mockResolvedValue(undefined),
        writeFile: vi.fn().mockResolvedValue(undefined)
    })
}));

vi.mock('../../src/constants', () => ({
    DEFAULT_EXCLUDED_PATTERNS: ['*.test.ts'],
    DEFAULT_FROM_COMMIT_ALIAS: 'main',
    DEFAULT_TO_COMMIT_ALIAS: 'HEAD',
    DEFAULT_OUTPUT_DIRECTORY: 'output',
    DEFAULT_MAX_DIFF_BYTES: 2048
}));

vi.mock('../../src/util/interactive', () => ({
    getUserChoice: vi.fn().mockResolvedValue('c'), // Default to confirm
    editContentInEditor: vi.fn().mockResolvedValue({
        content: 'Edited Title\n\nEdited body content',
        wasEdited: true
    }),
    improveContentWithLLM: vi.fn().mockResolvedValue({
        title: 'Improved title',
        body: 'Improved body content'
    }),
    getLLMFeedbackInEditor: vi.fn().mockResolvedValue('Make it more detailed'),
    requireTTY: vi.fn(), // Mock doesn't throw by default
    STANDARD_CHOICES: {
        CONFIRM: { key: 'c', label: 'Confirm and proceed' },
        EDIT: { key: 'e', label: 'Edit in editor' },
        SKIP: { key: 's', label: 'Skip and abort' },
        IMPROVE: { key: 'i', label: 'Improve with LLM feedback' }
    }
}));

vi.mock('@riotprompt/riotprompt', () => ({
    Formatter: {
        create: vi.fn().mockReturnValue({
            formatPrompt: vi.fn().mockReturnValue({
                messages: [{ role: 'user', content: 'mock message' }]
            })
        })
    }
}));

vi.mock('../../src/util/git', () => ({
    getDefaultFromRef: vi.fn().mockResolvedValue('main')
}));

describe('release command', () => {
    let Release: any;
    let mockStorage: any;
    let mockValidation: any;
    let mockInteractive: any;
    let mockOpenai: any;
    let mockLog: any;
    let mockDiff: any;
    let mockReleasePrompt: any;
    let mockGithub: any;

    beforeAll(async () => {
        Release = await import('../../src/commands/release');

        // Get access to mocked modules for test configuration
        mockStorage = await import('../../src/util/storage');
        mockValidation = await import('../../src/util/validation');
        mockInteractive = await import('../../src/util/interactive');
        mockOpenai = await import('../../src/util/openai');
        mockLog = await import('../../src/content/log');
        mockDiff = await import('../../src/content/diff');
        mockReleasePrompt = await import('../../src/prompt/release');
        mockGithub = await import('../../src/util/github');
    });

    beforeEach(async () => {
        vi.clearAllMocks();
    });

    afterEach(() => {
        vi.clearAllMocks();
    });

    it('should execute release command with default parameters', async () => {
        const runConfig = {
            model: 'gpt-4'
        };

        const result = await Release.execute(runConfig);

        expect(result).toEqual({
            title: 'mock title',
            body: 'mock body'
        });
    });

    it('should execute release command with custom parameters', async () => {
        const runConfig = {
            model: 'gpt-4',
            release: {
                from: 'v1.0.0',
                to: 'main'
            }
        };

        const result = await Release.execute(runConfig);

        expect(result).toEqual({
            title: 'mock title',
            body: 'mock body'
        });
    });

    it('should handle dry run mode', async () => {
        const runConfig = {
            model: 'gpt-4',
            dryRun: true
        };

        const result = await Release.execute(runConfig);

        expect(result).toEqual({
            title: 'mock title',
            body: 'mock body'
        });
    });

    it('should handle custom excluded patterns', async () => {
        const runConfig = {
            model: 'gpt-4',
            excludedPatterns: ['*.spec.ts', '*.test.ts']
        };

        const result = await Release.execute(runConfig);

        expect(result).toEqual({
            title: 'mock title',
            body: 'mock body'
        });
    });

    it('should handle custom output directory', async () => {
        const runConfig = {
            model: 'gpt-4',
            outputDirectory: 'custom-output'
        };

        const result = await Release.execute(runConfig);

        expect(result).toEqual({
            title: 'mock title',
            body: 'mock body'
        });
    });

    it('should handle debug mode', async () => {
        const runConfig = {
            model: 'gpt-4',
            debug: true,
            outputDirectory: 'debug-output'
        };

        const result = await Release.execute(runConfig);

        expect(result).toEqual({
            title: 'mock title',
            body: 'mock body'
        });
    });

    it('should handle release focus and context', async () => {
        const runConfig = {
            model: 'gpt-4',
            release: {
                focus: 'bug fixes',
                context: 'hotfix release'
            },
            contextDirectories: ['src', 'tests']
        };

        const result = await Release.execute(runConfig);

        expect(result).toEqual({
            title: 'mock title',
            body: 'mock body'
        });
    });

    it('should handle discovered config directories and overrides', async () => {
        const runConfig = {
            model: 'gpt-4',
            discoveredConfigDirs: ['config1', 'config2'],
            overrides: true
        };

        const result = await Release.execute(runConfig);

        expect(result).toEqual({
            title: 'mock title',
            body: 'mock body'
        });
    });

    it('should handle complex configuration with all optional parameters', async () => {
        const runConfig = {
            model: 'gpt-4',
            dryRun: true,
            debug: true,
            outputDirectory: 'custom-output',
            excludedPatterns: ['*.spec.ts'],
            discoveredConfigDirs: ['config1', 'config2'],
            overrides: true,
            contextDirectories: ['src', 'lib'],
            release: {
                from: 'v1.0.0',
                to: 'v1.1.0',
                focus: 'performance improvements',
                context: 'quarterly release'
            }
        };

        const result = await Release.execute(runConfig);

        expect(result).toEqual({
            title: 'mock title',
            body: 'mock body'
        });
    });

    describe('Interactive Release Functionality', () => {
        it('should run the LLM improvement pipeline and apply improved content', async () => {
            // Seed initial release generation
            mockOpenai.createCompletionWithRetry.mockResolvedValueOnce({
                title: 'Base title',
                body: 'Base body'
            });

            // Feedback then improved content
            mockInteractive.getUserChoice
                .mockResolvedValueOnce('i') // choose improve
                .mockResolvedValueOnce('c'); // then confirm

            mockInteractive.getLLMFeedbackInEditor.mockResolvedValue('Tighten wording');

            // LLM improvement call returns improved notes
            mockOpenai.createCompletionWithRetry.mockResolvedValueOnce({
                title: 'Improved title',
                body: 'Improved body content'
            });

            const runConfig = {
                model: 'gpt-4',
                release: { interactive: true }
            };

            const result = await Release.execute(runConfig);

            expect(result).toEqual({ title: 'Improved title', body: 'Improved body content' });
        });
        it('should handle interactive mode with confirm action', async () => {
            mockInteractive.getUserChoice.mockResolvedValue('c'); // Confirm
            // Ensure base completion returns baseline content for this test and clear any prior once-queues
            mockOpenai.createCompletionWithRetry.mockReset();
            mockOpenai.createCompletionWithRetry.mockResolvedValue({
                title: 'mock title',
                body: 'mock body'
            });

            const runConfig = {
                model: 'gpt-4',
                release: {
                    interactive: true
                }
            };

            const result = await Release.execute(runConfig);

            expect(mockInteractive.requireTTY).toHaveBeenCalled();
            expect(mockInteractive.getUserChoice).toHaveBeenCalledWith(
                '\nWhat would you like to do with these release notes?',
                expect.arrayContaining([
                    mockInteractive.STANDARD_CHOICES.CONFIRM,
                    mockInteractive.STANDARD_CHOICES.EDIT,
                    mockInteractive.STANDARD_CHOICES.SKIP,
                    mockInteractive.STANDARD_CHOICES.IMPROVE
                ]),
                expect.objectContaining({
                    nonTtyErrorSuggestions: ['Use --dry-run to see the generated content without interaction']
                })
            );
            expect(result).toEqual({
                title: 'mock title',
                body: 'mock body'
            });
        });

        it('should handle interactive mode with edit action', async () => {
            mockInteractive.getUserChoice
                .mockResolvedValueOnce('e') // Edit first
                .mockResolvedValueOnce('c'); // Then confirm

            const runConfig = {
                model: 'gpt-4',
                release: {
                    interactive: true
                }
            };

            const result = await Release.execute(runConfig);

            expect(mockInteractive.editContentInEditor).toHaveBeenCalledWith(
                'mock title\n\nmock body',
                expect.arrayContaining([
                    '# Edit your release notes below. Lines starting with "#" will be ignored.',
                    '# The first line is the title, everything else is the body.',
                    '# Save and close the editor when you are done.'
                ]),
                '.md'
            );
            expect(result).toEqual({
                title: 'Edited Title',
                body: 'Edited body content'
            });
        });

        it('should handle interactive mode with skip action', async () => {
            mockInteractive.getUserChoice.mockResolvedValue('s'); // Skip

            const runConfig = {
                model: 'gpt-4',
                release: {
                    interactive: true
                }
            };

            const result = await Release.execute(runConfig);

            expect(result).toEqual({
                title: 'mock title',
                body: 'mock body'
            });
        });

        it('should handle interactive mode with improve action', async () => {
            mockInteractive.getUserChoice
                .mockResolvedValueOnce('i') // Improve first
                .mockResolvedValueOnce('c'); // Then confirm

            const runConfig = {
                model: 'gpt-4',
                release: {
                    interactive: true
                }
            };

            const result = await Release.execute(runConfig);

            expect(mockInteractive.getLLMFeedbackInEditor).toHaveBeenCalledWith(
                'release notes',
                'mock title\n\nmock body'
            );
            expect(mockInteractive.improveContentWithLLM).toHaveBeenCalled();
            expect(result).toEqual({
                title: 'Improved title',
                body: 'Improved body content'
            });
        });

        it('should handle edit errors gracefully', async () => {
            mockInteractive.getUserChoice
                .mockResolvedValueOnce('e') // Edit first (fails)
                .mockResolvedValueOnce('c'); // Then confirm
            mockInteractive.editContentInEditor.mockRejectedValueOnce(new Error('Editor failed'));

            const runConfig = {
                model: 'gpt-4',
                release: {
                    interactive: true
                }
            };

            const result = await Release.execute(runConfig);

            // Should continue and allow user to try again
            expect(result).toEqual({
                title: 'mock title',
                body: 'mock body'
            });
        });

        it('should handle LLM improvement errors gracefully', async () => {
            mockInteractive.getUserChoice
                .mockResolvedValueOnce('i') // Improve first (fails)
                .mockResolvedValueOnce('c'); // Then confirm
            mockInteractive.improveContentWithLLM.mockRejectedValueOnce(new Error('LLM failed'));

            const runConfig = {
                model: 'gpt-4',
                release: {
                    interactive: true
                }
            };

            const result = await Release.execute(runConfig);

            // Should continue and allow user to try again
            expect(result).toEqual({
                title: 'mock title',
                body: 'mock body'
            });
        });

        it('should not enter interactive mode when dry run is enabled', async () => {
            const runConfig = {
                model: 'gpt-4',
                dryRun: true,
                release: {
                    interactive: true
                }
            };

            const result = await Release.execute(runConfig);

            expect(mockInteractive.requireTTY).not.toHaveBeenCalled();
            expect(mockInteractive.getUserChoice).not.toHaveBeenCalled();
            expect(result).toEqual({
                title: 'mock title',
                body: 'mock body'
            });
        });
    });

    describe('Error Handling', () => {
        it('should handle validation errors', async () => {
            mockValidation.validateReleaseSummary.mockImplementation(() => {
                throw new Error('Invalid release summary');
            });

            const runConfig = { model: 'gpt-4' };

            await expect(Release.execute(runConfig)).rejects.toThrow('Invalid release summary');
        });

        it('should handle log creation errors', async () => {
            mockLog.create.mockReturnValue({
                get: vi.fn().mockRejectedValue(new Error('Git log failed'))
            });

            const runConfig = { model: 'gpt-4' };

            await expect(Release.execute(runConfig)).rejects.toThrow('Git log failed');
        });

        it('should handle diff creation errors', async () => {
            mockDiff.create.mockReturnValue({
                get: vi.fn().mockRejectedValue(new Error('Git diff failed'))
            });

            const runConfig = { model: 'gpt-4' };

            await expect(Release.execute(runConfig)).rejects.toThrow('Git diff failed');
        });

        it('should handle prompt creation errors', async () => {
            // Reset mocks to ensure log and diff succeed
            mockLog.create.mockReturnValue({
                get: vi.fn().mockResolvedValue('mock log content')
            });
            mockDiff.create.mockReturnValue({
                get: vi.fn().mockResolvedValue('mock diff content')
            });

            mockReleasePrompt.createPrompt.mockRejectedValue(new Error('Prompt creation failed'));

            const runConfig = { model: 'gpt-4' };

            await expect(Release.execute(runConfig)).rejects.toThrow('Prompt creation failed');
        });

        it('should handle LLM API errors', async () => {
            // Reset mocks to ensure log and diff succeed
            mockLog.create.mockReturnValue({
                get: vi.fn().mockResolvedValue('mock log content')
            });
            mockDiff.create.mockReturnValue({
                get: vi.fn().mockResolvedValue('mock diff content')
            });

            // Reset validation mock to success
            mockValidation.validateReleaseSummary.mockImplementation((data: any) => data);

            // Reset prompt mock to success
            mockReleasePrompt.createPrompt.mockResolvedValue({
                prompt: 'mock prompt',
                isLargeRelease: false,
                maxTokens: 4000
            });

            mockOpenai.createCompletionWithRetry.mockRejectedValue(new Error('OpenAI API failed'));

            const runConfig = { model: 'gpt-4' };

            await expect(Release.execute(runConfig)).rejects.toThrow('OpenAI API failed');
        });

        it('should handle storage errors when saving timestamped file', async () => {
            // Reset mocks to ensure log and diff succeed
            mockLog.create.mockReturnValue({
                get: vi.fn().mockResolvedValue('mock log content')
            });
            mockDiff.create.mockReturnValue({
                get: vi.fn().mockResolvedValue('mock diff content')
            });

            // Reset validation mock to success
            mockValidation.validateReleaseSummary.mockImplementation((data: any) => data);

            // Reset prompt mock to success
            mockReleasePrompt.createPrompt.mockResolvedValue({
                prompt: 'mock prompt',
                isLargeRelease: false,
                maxTokens: 4000
            });

            // Reset OpenAI mock to success
            mockOpenai.createCompletionWithRetry.mockResolvedValue({
                title: 'mock title',
                body: 'mock body'
            });

            const mockStorageInstance = {
                ensureDirectory: vi.fn().mockResolvedValue(undefined),
                writeFile: vi.fn().mockRejectedValue(new Error('Disk full'))
            };
            mockStorage.create.mockReturnValue(mockStorageInstance);

            const runConfig = { model: 'gpt-4' };

            // Should not throw error, just log warning
            const result = await Release.execute(runConfig);
            expect(result).toEqual({
                title: 'mock title',
                body: 'mock body'
            });
        });
    });

    describe('Retry Callback Functionality', () => {
        it('should reduce diff size on retry attempts', async () => {
            // Reset mocks to ensure log and diff succeed
            mockLog.create.mockReturnValue({
                get: vi.fn().mockResolvedValue('mock log content')
            });
            mockDiff.create.mockReturnValue({
                get: vi.fn().mockResolvedValue('mock diff content')
            });

            // Reset validation mock to success
            mockValidation.validateReleaseSummary.mockImplementation((data: any) => data);

            // Reset prompt mock to success
            mockReleasePrompt.createPrompt.mockResolvedValue({
                prompt: 'mock prompt',
                isLargeRelease: false,
                maxTokens: 4000
            });

            let retryCount = 0;
            mockOpenai.createCompletionWithRetry.mockImplementation(async (messages: any, options: any, retryCallback: any) => {
                if (retryCallback && retryCount === 0) {
                    retryCount++;
                    // Simulate retry
                    const newMessages = await retryCallback(1);
                    expect(newMessages).toBeDefined();
                    expect(Array.isArray(newMessages)).toBe(true);
                }
                return { title: 'mock title', body: 'mock body' };
            });

            mockDiff.truncateDiffByFiles.mockReturnValue('truncated diff content');

            const runConfig = {
                model: 'gpt-4',
                release: {
                    maxDiffBytes: 2048
                }
            };

            const result = await Release.execute(runConfig);

            expect(result).toEqual({
                title: 'mock title',
                body: 'mock body'
            });
        });

        it('should handle progressive diff reduction on multiple retries', async () => {
            // Create a long mock diff content that will trigger truncation
            const longDiffContent = 'a'.repeat(5000); // 5000 chars, longer than maxDiffBytes

            // Reset mocks to ensure log and diff succeed
            mockLog.create.mockReturnValue({
                get: vi.fn().mockResolvedValue('mock log content')
            });
            mockDiff.create.mockReturnValue({
                get: vi.fn().mockResolvedValue(longDiffContent)
            });

            // Reset validation mock to success
            mockValidation.validateReleaseSummary.mockImplementation((data: any) => data);

            // Reset prompt mock to success
            mockReleasePrompt.createPrompt.mockResolvedValue({
                prompt: 'mock prompt',
                isLargeRelease: false,
                maxTokens: 4000
            });

            let retryCount = 0;
            mockOpenai.createCompletionWithRetry.mockImplementation(async (messages: any, options: any, retryCallback: any) => {
                if (retryCallback && retryCount < 2) {
                    retryCount++;
                    const newMessages = await retryCallback(retryCount);
                    expect(newMessages).toBeDefined();
                }
                return { title: 'mock title', body: 'mock body' };
            });

            mockDiff.truncateDiffByFiles.mockReturnValue('increasingly truncated diff');

            const runConfig = {
                model: 'gpt-4',
                release: {
                    maxDiffBytes: 4096
                }
            };

            await Release.execute(runConfig);

            // Should call truncateDiffByFiles at least once during retries.
            // The retry callback is invoked once within the mocked call, so a single truncation is expected.
            expect(mockDiff.truncateDiffByFiles).toHaveBeenCalledTimes(1);
        });
    });

    describe('Milestones Integration', () => {
        it('should incorporate milestone issues and include versions from package.json and publish target', async () => {
            // Make storage provide a package.json with a dev version
            const mockStorageInstance = {
                ensureDirectory: vi.fn().mockResolvedValue(undefined),
                writeFile: vi.fn().mockResolvedValue(undefined),
                readFile: vi.fn().mockResolvedValue('{"name":"pkg","version":"1.2.4-dev.0"}')
            };
            mockStorage.create.mockReturnValue(mockStorageInstance);

            // Return milestone content
            mockGithub.getMilestoneIssuesForRelease.mockResolvedValue('milestone content');

            const runConfig = {
                model: 'gpt-4',
                publish: { targetVersion: '1.2.5' }
            };

            await Release.execute(runConfig);

            expect(mockGithub.getMilestoneIssuesForRelease).toHaveBeenCalledWith(
                expect.arrayContaining(['1.2.5']),
                50000
            );

            // Ensure milestone content flows into prompt
            expect(mockReleasePrompt.createPrompt).toHaveBeenCalledWith(
                expect.anything(),
                expect.objectContaining({ milestoneIssues: 'milestone content' }),
                expect.anything()
            );
        });

        it('should skip milestone lookup when disabled', async () => {
            mockGithub.getMilestoneIssuesForRelease.mockClear();

            const runConfig = {
                model: 'gpt-4',
                release: { noMilestones: true }
            };

            await Release.execute(runConfig);

            expect(mockGithub.getMilestoneIssuesForRelease).not.toHaveBeenCalled();
        });

        it('should include current non-dev version when present in package.json', async () => {
            const mockStorageInstance = {
                ensureDirectory: vi.fn().mockResolvedValue(undefined),
                writeFile: vi.fn().mockResolvedValue(undefined),
                readFile: vi.fn().mockResolvedValue('{"name":"pkg","version":"2.0.0"}')
            };
            mockStorage.create.mockReturnValue(mockStorageInstance);

            mockGithub.getMilestoneIssuesForRelease.mockResolvedValue('milestone content');

            const runConfig = { model: 'gpt-4' };
            await Release.execute(runConfig);

            expect(mockGithub.getMilestoneIssuesForRelease).toHaveBeenCalledWith(
                expect.arrayContaining(['2.0.0']),
                50000
            );
        });

        it('should handle when no milestone issues are found', async () => {
            const mockStorageInstance = {
                ensureDirectory: vi.fn().mockResolvedValue(undefined),
                writeFile: vi.fn().mockResolvedValue(undefined),
                readFile: vi.fn().mockResolvedValue('{"name":"pkg","version":"1.0.0"}')
            };
            mockStorage.create.mockReturnValue(mockStorageInstance);

            // Return empty content to trigger the debug path
            mockGithub.getMilestoneIssuesForRelease.mockResolvedValue('');

            const runConfig = { model: 'gpt-4' };
            await Release.execute(runConfig);

            expect(mockGithub.getMilestoneIssuesForRelease).toHaveBeenCalled();
            // Also ensure prompt still receives structure with empty milestone issues
            expect(mockReleasePrompt.createPrompt).toHaveBeenCalledWith(
                expect.anything(),
                expect.objectContaining({ milestoneIssues: '' }),
                expect.anything()
            );
        });
    });

    describe('Configuration and Edge Cases', () => {
        it('should handle large release detection', async () => {
            // Reset mocks to ensure log and diff succeed
            mockLog.create.mockReturnValue({
                get: vi.fn().mockResolvedValue('mock log content')
            });
            mockDiff.create.mockReturnValue({
                get: vi.fn().mockResolvedValue('mock diff content')
            });

            // Reset validation mock to success
            mockValidation.validateReleaseSummary.mockImplementation((data: any) => data);

            mockReleasePrompt.createPrompt.mockResolvedValue({
                prompt: 'mock prompt',
                isLargeRelease: true,
                maxTokens: 8000
            });

            const runConfig = { model: 'gpt-4' };

            const result = await Release.execute(runConfig);

            expect(mockOpenai.createCompletionWithRetry).toHaveBeenCalledWith(
                expect.any(Array),
                expect.objectContaining({
                    maxTokens: 8000
                }),
                expect.any(Function)
            );
            expect(result).toEqual({
                title: 'mock title',
                body: 'mock body'
            });
        });

        it('should handle custom maxDiffBytes configuration', async () => {
            // Reset mocks to ensure log and diff succeed
            mockLog.create.mockReturnValue({
                get: vi.fn().mockResolvedValue('mock log content')
            });
            mockDiff.create.mockReturnValue({
                get: vi.fn().mockResolvedValue('mock diff content')
            });

            // Reset validation mock to success
            mockValidation.validateReleaseSummary.mockImplementation((data: any) => data);

            const runConfig = {
                model: 'gpt-4',
                release: {
                    maxDiffBytes: 1024
                }
            };

            await Release.execute(runConfig);

            expect(mockDiff.create).toHaveBeenCalledWith({
                from: 'main',
                to: 'HEAD',
                excludedPatterns: ['*.test.ts'],
                maxDiffBytes: 1024
            });
        });

        it('should handle messageLimit configuration', async () => {
            // Reset mocks to ensure log and diff succeed
            mockLog.create.mockReturnValue({
                get: vi.fn().mockResolvedValue('mock log content')
            });
            mockDiff.create.mockReturnValue({
                get: vi.fn().mockResolvedValue('mock diff content')
            });

            // Reset validation mock to success
            mockValidation.validateReleaseSummary.mockImplementation((data: any) => data);

            const runConfig = {
                model: 'gpt-4',
                release: {
                    messageLimit: 50
                }
            };

            await Release.execute(runConfig);

            expect(mockLog.create).toHaveBeenCalledWith({
                from: 'main',
                to: 'HEAD',
                limit: 50
            });
        });

        it('should save timestamped release notes file', async () => {
            // Reset mocks to ensure log and diff succeed
            mockLog.create.mockReturnValue({
                get: vi.fn().mockResolvedValue('mock log content')
            });
            mockDiff.create.mockReturnValue({
                get: vi.fn().mockResolvedValue('mock diff content')
            });

            // Reset validation mock to success
            mockValidation.validateReleaseSummary.mockImplementation((data: any) => data);

            const mockStorageInstance = {
                ensureDirectory: vi.fn().mockResolvedValue(undefined),
                writeFile: vi.fn().mockResolvedValue(undefined)
            };
            mockStorage.create.mockReturnValue(mockStorageInstance);

            const runConfig = {
                model: 'gpt-4',
                outputDirectory: 'test-output'
            };

            await Release.execute(runConfig);

            expect(mockStorageInstance.ensureDirectory).toHaveBeenCalledWith('test-output');
            expect(mockStorageInstance.writeFile).toHaveBeenCalledWith(
                'test-output/release-notes-123456.md',
                '# mock title\n\nmock body',
                'utf-8'
            );
        });

        it('should handle missing release configuration gracefully', async () => {
            // Reset mocks to ensure log and diff succeed
            mockLog.create.mockReturnValue({
                get: vi.fn().mockResolvedValue('mock log content')
            });
            mockDiff.create.mockReturnValue({
                get: vi.fn().mockResolvedValue('mock diff content')
            });

            // Reset validation mock to success
            mockValidation.validateReleaseSummary.mockImplementation((data: any) => data);

            const runConfig = {
                model: 'gpt-4'
                // No release configuration
            };

            const result = await Release.execute(runConfig);

            expect(mockLog.create).toHaveBeenCalledWith({
                from: 'main',
                to: 'HEAD',
                limit: undefined
            });
            expect(mockDiff.create).toHaveBeenCalledWith({
                from: 'main',
                to: 'HEAD',
                excludedPatterns: ['*.test.ts'],
                maxDiffBytes: 2048
            });
            expect(result).toEqual({
                title: 'mock title',
                body: 'mock body'
            });
        });
    });

    describe('TTY Requirements', () => {
        it('should throw error when interactive mode requires TTY', async () => {
            // Reset mocks to ensure log and diff succeed
            mockLog.create.mockReturnValue({
                get: vi.fn().mockResolvedValue('mock log content')
            });
            mockDiff.create.mockReturnValue({
                get: vi.fn().mockResolvedValue('mock diff content')
            });

            // Reset validation mock to success
            mockValidation.validateReleaseSummary.mockImplementation((data: any) => data);

            mockInteractive.requireTTY.mockImplementation(() => {
                throw new Error('Interactive mode requires a terminal. Use --dry-run instead.');
            });

            const runConfig = {
                model: 'gpt-4',
                release: {
                    interactive: true
                }
            };

            await expect(Release.execute(runConfig)).rejects.toThrow(
                'Interactive mode requires a terminal. Use --dry-run instead.'
            );
        });
    });
});
